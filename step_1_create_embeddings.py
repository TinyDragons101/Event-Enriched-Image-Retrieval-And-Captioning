import argparse
import os
from tqdm import tqdm
import torch
from internvl import CustonInternVLRetrievalModel

def main():
    parser = argparse.ArgumentParser(description="Generate image embeddings using CustonInternVLRetrievalModel.")
    parser.add_argument("--part", type=int, default=None, help="Part number to process (1-based index).")
    parser.add_argument("--total_parts", type=int, default=4, help="Total number of parts to split workload.")
    parser.add_argument("--device", type=str, default="cuda:0", help="Torch device (e.g., 'cuda:0', 'cpu').")
    parser.add_argument("--input_folder", type=str, default="./data/database/database_origin/database_img/", help="Folder containing input images.")
    parser.add_argument("--output_folder", type=str, default="./embeddings/database/", help="Folder to save output embeddings.")

    args = parser.parse_args()
    device = torch.device(args.device if torch.cuda.is_available() or "cpu" in args.device else "cpu")

    if not os.path.exists(args.output_folder):
        os.makedirs(args.output_folder)

    with torch.no_grad():
        embedding_model = CustonInternVLRetrievalModel(device=device)

        # Get image list
        image_paths = [os.path.join(args.input_folder, f)
                       for f in os.listdir(args.input_folder)
                       if f.lower().endswith(('.jpg', '.jpeg', '.png'))]
        image_paths.sort()

        # Split
        if args.part is not None:
            assert 1 <= args.part <= args.total_parts, "part must be between 1 and total_parts"
            total = len(image_paths)
            split_size = total // args.total_parts
            start = (args.part - 1) * split_size
            end = total if args.part == args.total_parts else start + split_size
            image_subset = image_paths[start:end]
            print(f"ðŸ”¹ Running Part {args.part}/{args.total_parts}: Processing {len(image_subset)} images")
        else:
            image_subset = image_paths
            print(f"ðŸ”¹ Running Full Mode: Processing all {len(image_subset)} images")
            
        # Encode all images with batch
        embeddings = embedding_model.encode_image(
            image_subset,
            is_path=True,
        )

        for path, emb in zip(image_subset, embeddings):
            name = os.path.splitext(os.path.basename(path))[0]
            output_path = os.path.join(args.output_folder, f"{name}.pt")
            torch.save(emb, output_path)
        
if __name__ == "__main__":
    main()
